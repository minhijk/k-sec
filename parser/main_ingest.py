# main_ingest.py (íŒŒì‹± + í†µí•© ë²„ì „)

import json
import os
from typing import Dict, Any, List

# --- 1. ëª¨ë“ˆ ì„í¬íŠ¸ ---
# ê° ë¬¸ì„œ íŒŒì„œì™€ ìµœì¢… í†µí•© í•¨ìˆ˜ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from parsers import cis_parser, enisa_parser, nist_parser
from combine_parsers import unify_json

# --- 2. ì„¤ì •: ì²˜ë¦¬í•  ë¬¸ì„œì™€ íŒŒì„œ ì •ì˜ ---
DOCUMENT_SOURCES = [
    {
        # "parser/" ê²½ë¡œ ì¶”ê°€
        "path": "parser/source_documents/CIS_Kubernetes_Benchmark_V1.12_PDF.pdf",
        "parser": cis_parser.parse, 
        "output_file": "structured_cis.json"
    },
    {
        # "parser/" ê²½ë¡œ ì¶”ê°€
        "path": "parser/source_documents/ENISA_Technical_implementation_guidance_on_cybersecurity_risk_management_measures_version_1.0.pdf",
        "parser": enisa_parser.parse,
        "output_file": "structured_enisa.json"
    },
    {
        # "parser/" ê²½ë¡œ ì¶”ê°€
        "path": "parser/source_documents/NIST.SP.800-190.pdf",
        "parser": nist_parser.parse,
        "output_file": "structured_nist.json"
    }
]


def run_ingestion_pipeline():
    """ë°ì´í„° íŒŒì‹±ë¶€í„° ìµœì¢… í†µí•©ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    # ì¶œë ¥ ê²½ë¡œë¥¼ "parser/parsers_output"ìœ¼ë¡œ ìˆ˜ì •
    output_dir = os.path.join("parser", "parsers_output")
    os.makedirs(output_dir, exist_ok=True)
    
    # --- 1ë‹¨ê³„: ê°œë³„ ë¬¸ì„œ íŒŒì‹± ---
    print("ğŸš€ 1ë‹¨ê³„: ë™ì  ìŠ¤í‚¤ë§ˆ íŒŒì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    successful_parses = []
    for source in DOCUMENT_SOURCES:
        file_path = source["path"]
        print(f"\n-> ğŸ“„ '{file_path}' íŒŒì‹± ì¤‘...")
        
        if not os.path.exists(file_path):
            print(f"   âŒ íŒŒì¼ ì—†ìŒ: '{file_path}'. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        
        data = source["parser"](file_path)
        
        if data:
            output_path = os.path.join(output_dir, source["output_file"])
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            print(f"   âœ… íŒŒì‹± ì„±ê³µ! {len(data)}ê°œ í•­ëª© -> '{output_path}'ì— ì €ì¥ ì™„ë£Œ")
            successful_parses.append(output_path)
        else:
            print(f"   âŒ íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    print("\n\nğŸ‰ ì„ íƒëœ ë¬¸ì„œì˜ ê°œë³„ íŒŒì‹±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # --- 2ë‹¨ê³„: íŒŒì‹±ëœ JSON íŒŒì¼ í†µí•© ---
    print("\nğŸš€ 2ë‹¨ê³„: íŒŒì‹±ëœ JSON íŒŒì¼ë“¤ì„ ë‹¨ì¼ íŒŒì¼ë¡œ í†µí•©í•©ë‹ˆë‹¤...")
    
    # unify_json í•¨ìˆ˜ì— í•„ìš”í•œ íŒŒì¼ ê²½ë¡œë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.
    nist_path = os.path.join(output_dir, "structured_nist.json")
    cis_path = os.path.join(output_dir, "structured_cis.json")
    enisa_path = os.path.join(output_dir, "structured_enisa.json")
    unified_output_path = "structured_all.json"

    # ëª¨ë“  ì†ŒìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ í›„ í†µí•© í•¨ìˆ˜ í˜¸ì¶œ
    required_files = [nist_path, cis_path, enisa_path]
    if all(os.path.exists(p) for p in required_files):
        unify_json(
            nist_path=nist_path,
            cis_path=cis_path,
            enisa_path=enisa_path,
            output_path=unified_output_path
        )
    else:
        print(f"   âŒ í†µí•© ì‹¤íŒ¨: í•„ìš”í•œ JSON íŒŒì¼ ì¤‘ ì¼ë¶€ê°€ ì—†ìŠµë‹ˆë‹¤. 1ë‹¨ê³„ íŒŒì‹± ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        missing_files = [f for f in required_files if not os.path.exists(f)]
        print(f"   (ëˆ„ë½ëœ íŒŒì¼: {', '.join(missing_files)})")
    
    print("\n\nğŸ‰ ëª¨ë“  ë¬¸ì„œì˜ íŒŒì‹± ë° í†µí•©ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    run_ingestion_pipeline()